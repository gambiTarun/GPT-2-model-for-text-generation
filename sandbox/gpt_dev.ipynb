{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9366a20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0   415k      0  0:00:02  0:00:02 --:--:--  415k\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt --output input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8272bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d84edaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of chars in input:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of chars in input: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5a4bfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20380226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f5bfc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 8]\n",
      "hello world.\n"
     ]
    }
   ],
   "source": [
    "# character encoding and decoding\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda x: [stoi[s] for s in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "print(encode(\"hello world.\"))\n",
    "print(decode(encode(\"hello world.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daf1b5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1ad9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and val\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "Val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "232e4bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8    # context length\n",
    "# the following chunk of data include block_size of data points. \n",
    "# In the sense that when 18 is the input, model has to predict 47.\n",
    "# When [18, 47] is the input, model has to predict 56. When \n",
    "# [18, 47, 56] is the input, model has to predict 57... and so on.\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "699ca354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), the target is 47\n",
      "when input is tensor([18, 47]), the target is 56\n",
      "when input is tensor([18, 47, 56]), the target is 57\n",
      "when input is tensor([18, 47, 56, 57]), the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(f\"when input is {context}, the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e71aa2",
   "metadata": {},
   "source": [
    "This way of feeding all possible context lengths upto block_size allows the model to learn how to behave in any given context length upto block_size during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f9c48bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "# creating batches of input and targets\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split, device=\"cpu\"):\n",
    "    data = train_data if split==\"train\" else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef96c15a",
   "metadata": {},
   "source": [
    "## Simple Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "78b11184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e371bc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.6485, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SP MgD&GM .3YCKf fRwaX$V&tt3s!muDn-oivDTV?s!!q.\n",
      "pTQ3!uLT;ehcL.PJgOwW\n",
      "RlyE$k!MXIBL;;FGZOrc!\n",
      "jHA;Rq.?,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # a simple lookup table of vocab_size (keys) and vocab_size (value dim)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        # idx and targets are both (B,T) tensors of integers, ie: batch, time\n",
    "        logits = self.token_embedding_table(idx) # outputs: (B,T,C)\n",
    "        \n",
    "        if targets==None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices for the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last token (timestep)\n",
    "            logits = logits[:, -1, :]  # becomes (B, C) from (B, T, C) (since targets wasnt passed, the shape is btc)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "        return idx\n",
    "    \n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "xb, yb = get_batch(\"train\")\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "        \n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9ef02f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer for training\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5181c6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3848743438720703\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    \n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4f5d62ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wr\n",
      "\n",
      "WAy s mee do mpoweraitass and\n",
      "\n",
      "NCEve ser.\n",
      "'sh ldy m.\n",
      "\n",
      "GIONo ou gh f, f my o hoteato hit ypefowe the sthenafr Mourcuss, tur ich thaknthe leis fe thin anay\n",
      "DUCambe thene h nd ld t thitr nothuso s phey?\n",
      "\n",
      "Wenfasarin\n",
      "BRYondfitho, grey.\n",
      "TAUCourer oueangredw\n",
      "BUCHextibl my, le ove d ispaprry fed iss\n",
      "G che sshewhesthot br couse meagur inomasthayo he;\n",
      "Ands'd, brourd d's fthece, co bes ithitinfivinod s owhtowe med ior hiss Gadortod ses bu k, d'thiloo ft my oorischotl d wheeyodsdust w If; r areshee I ncat.\n",
      "\n",
      "The be awis. bu kne?\n",
      "F ke Of f my bl t oard t I s, sutourd baint w\n",
      "ICETheseno otos\n",
      "NVINENGBu.\n",
      "And cin?\n",
      "lan oned'd\n",
      "BETyold w, l amay,\n",
      "So'dw,\n",
      "KI:\n",
      "Ifo th, thag w sto\n",
      "\n",
      "Yororr, cke ewe ano suthertrmakigheitherbucl br pt t ase n s ate icod gal!--ar MONREL:\n",
      "\n",
      "\n",
      "on tooury swno th heere o he:\n",
      "tswot my; je iveepre tasent, d dod m, inthooriprellingnche; d F pelerindn---s sunthadealf d, yomarof mol s'le,\n",
      "\n",
      "Yof, fad\n",
      "ILe, angasurisund pe tus hy bedusoudesy bezer sar g pave.\n",
      "\n",
      "My bugnoowamy d.\n",
      "Asespunqu t wec\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e1199",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009845f",
   "metadata": {},
   "source": [
    "### mathematical trick for self attention that increases efficiency using vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2cdf8d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6645f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b][:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1d7c0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones((T,T)))\n",
    "# we start with zero/constant affinity for future and past tokens\n",
    "wei = torch.zeros((T,T))\n",
    "# we set the future tokens affinity to -inf as they do not contribute to the prediciton\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\"))\n",
    "# we normalize the weights across the context length\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# we get the average attention of all the past tokens for the current token as follows\n",
    "# (T, T) @ (B, T, C) -> (B, T, T) @ (B, T, C) -> (B, T, C) (broadcasting happens in step 1)\n",
    "xbow2 = wei @ x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "be5648e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow,xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03e506d",
   "metadata": {},
   "source": [
    "### self-attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5e1cdc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# head params\n",
    "head_size = 16  # H\n",
    "key = nn.Linear(C, head_size, bias=False)    # weights for the key, ie: information possessed by the token\n",
    "query = nn.Linear(C, head_size, bias=False)  # weights for the query, ie: information seeked by the token\n",
    "value = nn.Linear(C, head_size, bias=False)  # weights for the value, ie: information communicated by the token\n",
    "k = key(x) # (B, T, H) the input data is fed into the weights to get the key vector\n",
    "q = query(x) # (B, T, H) the input data is fed into the weights to get the query vector\n",
    "v = value(x) # (B, T, H) the input data is fed into the weights to get the value vector\n",
    "\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, H) @ (B, H, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones((T,T)))\n",
    "# Masking the future tokens\n",
    "wei = wei.masked_fill(tril==0, float('-inf')) # (B, T, T)\n",
    "wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "\n",
    "out = wei @ v # # (B, T, T) @ (B, T, H) -> (B, T, H)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa9103",
   "metadata": {},
   "source": [
    "My Notes:\n",
    "\n",
    "- This masking is done in the Decoder Attention block meaning we only look at previous tokens in an autoregressive manner. \n",
    "\n",
    "- In an Encoder Attention block we will have information flowing from  past and future for learning the knowledge base of data. This knowledge base is referred to in the Decoder block in the form of \"Cross-Attention\" ([query(x)@key(y)]value(y)) as opposed to \"Self-Attention\"([query(x)@key(x)]value(x)). \n",
    "\n",
    "- Encoder Attention block -> Self-Attention ; Past+Future context\n",
    "- Decoder Attention block -> Cross-Attention ; Past context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5847a6f7",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "\n",
    "- \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "992b8da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn((B, T, head_size))\n",
    "q = torch.randn((B, T, head_size))\n",
    "wei = q @ k.transpose(-2,-1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3ec2ab28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0632)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ee1994d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9891)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cccf9b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9755)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7637f25b",
   "metadata": {},
   "source": [
    "- Adding the scaling factor for \"scaled attention\" avoids the peaky average property of softmax. Softmax is very sensitive to scale, so two vectors with same uniform distribution but different scales can give very pointy/peaky softmax output for a higher scaled input.\n",
    "\n",
    "- Softmax will converge towards One-Hot-Vectors, so it is important for the scaling to be uniform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cf27c440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1799, 0.1333, 0.2197, 0.2684, 0.1988])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, 0.5, 0.2]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a886099f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3195e-16, 4.3596e-28, 1.5230e-08, 1.0000e+00, 1.8795e-12])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, 0.5, 0.2])*90, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0aedc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b40c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
